import pandas as pd
from datasets import load_from_disk
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
from pathlib import Path
import os
import json
import re
import time
from typing import List, Dict

class InferenceEngine:
    """Inference engine for generating summaries"""
    def __init__(self, model_path: str, use_optimized: bool = True):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self._load_model(use_optimized)

    def _load_model(self, use_optimized: bool):
        """Load the best available model version"""
        print("üîç Searching for best available model...")
        model_candidates = []
        if use_optimized:
            base_path = Path(self.model_path).parent
            optimized_candidates = [
                (base_path / 'optimized_model' / 'fp16', 'FP16 Optimized'),
                (base_path / 'optimized_model' / 'quantized', 'Quantized'),
            ]
            for path, name in optimized_candidates:
                if path.exists() and any(path.iterdir()):
                    model_candidates.append((str(path), name))
        model_candidates.append((self.model_path, 'Original'))
        for model_path, model_name in model_candidates:
            try:
                print(f"üîÑ Trying to load {model_name} model from: {model_path}")
                self.model = T5ForConditionalGeneration.from_pretrained(model_path)
                self.tokenizer = T5Tokenizer.from_pretrained(model_path)
                self.model.to(self.device)
                self.model.eval()
                print(f"‚úÖ Successfully loaded {model_name} model")
                return
            except Exception as e:
                print(f"‚ùå Failed to load {model_name}: {e}")
                continue
        raise RuntimeError("‚ùå Could not load any model version!")

    def generate_summary(self, prompt: str, generation_config: Dict = None) -> str:
        """Generate summary with enhanced parameters for longer outputs"""
        if generation_config is None:
            # ENHANCED GENERATION CONFIG FOR LONGER PREDICTIONS
            generation_config = {
                'max_length': 400,           # Increased from 256
                'min_length': 70,            # Minimum length
                'num_beams': 6,              # Increased from 4 for better quality
                'early_stopping': False,     # Changed - Let it generate more content
                'do_sample': False,          # Keep deterministic
                'repetition_penalty': 1.15,  # Reduced from 1.2 to allow more medical repetition
                'length_penalty': 1.5,       # Strongly favor longer outputs
                'no_repeat_ngram_size': 2,   # Reduced from 3 for more flexibility
                'forced_bos_token_id': None,
                'forced_eos_token_id': None,
                'exponential_decay_length_penalty': None,
                'suppress_tokens': None,
                'begin_suppress_tokens': None,
            }
        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors='pt',
                truncation=True,
                padding='max_length',
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            print(f"üîß Using enhanced generation config: max_len={generation_config['max_length']}, min_len={generation_config['min_length']}, beams={generation_config['num_beams']}, length_penalty={generation_config['length_penalty']}")
            
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    **generation_config
                )
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            normalized_summary = self._normalize_for_evaluation(generated_text)
            
            # Log length for monitoring
            word_count = len(normalized_summary.split())
            if word_count < 50:
                print(f"‚ö†Ô∏è Short prediction ({word_count} words): {normalized_summary[:100]}...")
            elif word_count > 80:
                print(f"‚úÖ Good length prediction ({word_count} words)")
            
            return normalized_summary
        except Exception as e:
            print(f"‚ùåÔ∏è Failed to generate sample for prompt: {prompt[:50]}... Error: {e}")
            return "patient requires clinical assessment and appropriate treatment based on presenting symptoms and medical history"

    def _normalize_for_evaluation(self, summary: str) -> str:
        """Apply evaluation format normalization"""
        if not summary or summary.strip() == "":
            return "patient requires clinical assessment and appropriate treatment based on presenting symptoms and medical history"
        normalized = summary.lower()
        normalized = re.sub(r'[^\w\s]', '', normalized)
        normalized = re.sub(r'\s+', ' ', normalized)
        normalized = normalized.strip()
        if not normalized or len(normalized.split()) < 2:
            normalized = "patient requires clinical assessment and appropriate treatment based on presenting symptoms and medical history"
        return normalized

def run_inference(
        model_path='./model_outputs/final_model',
        test_path='outputs/test_dataset',
        output_path='outputs/submission.csv',
        use_optimized=True
):
    """Run inference with enhanced generation parameters for longer predictions"""
    print("üöÄ Starting Enhanced Inference with Longer Prediction Generation...")
    print("=" * 70)
    print("üîß PHASE 1 ENHANCEMENTS APPLIED:")
    print("‚úÖ Increased max_length: 256 ‚Üí 400")
    print("‚úÖ Added min_length: 60 words minimum")
    print("‚úÖ Enhanced num_beams: 4 ‚Üí 6")
    print("‚úÖ Optimized length_penalty: 1.0 ‚Üí 1.4")
    print("‚úÖ Disabled early_stopping for fuller generation")
    print("=" * 70)
    
    try:
        inference_engine = InferenceEngine(model_path, use_optimized)
        test_dataset = load_from_disk(test_path)
        print(f"‚úÖ Loaded {len(test_dataset)} test samples")
    except Exception as e:
        print(f"‚ùå Failed to initialize inference engine: {e}")
        raise
    print(f"üéØ Running enhanced inference on {len(test_dataset)} samples...")
    print(f"üíª Using device: {inference_engine.device}")
    
    predictions = []
    length_stats = []
    start_time = time.time()
    
    for i, example in enumerate(test_dataset):
        if i % 25 == 0:
            elapsed = time.time() - start_time
            if i > 0:
                eta = (elapsed / i) * (len(test_dataset) - i)
                avg_length = sum(length_stats) / len(length_stats) if length_stats else 0
                print(f"üìä Progress: {i+1}/{len(test_dataset)} ({100*(i+1)/len(test_dataset):.1f}%) - ETA: {eta/60:.1f}min - Avg Length: {avg_length:.1f} words")
        try:
            summary = inference_engine.generate_summary(example['Prompt'])
            predictions.append(summary)
            length_stats.append(len(summary.split()))
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing sample {i}: {e}")
            fallback = inference_engine._normalize_for_evaluation("patient requires clinical assessment and appropriate treatment based on presenting symptoms and medical history")
            predictions.append(fallback)
            length_stats.append(len(fallback.split()))
    
    total_time = time.time() - start_time
    avg_length = sum(length_stats) / len(length_stats)
    
    print(f"‚úÖ Enhanced inference completed in {total_time/60:.1f} minutes")
    print(f"‚ö° Average time per sample: {total_time/len(test_dataset):.3f}s")
    print(f"üìè ENHANCED AVERAGE LENGTH: {avg_length:.1f} words (Target: >75 words)")
    print(f"üìè Length range: {min(length_stats)} - {max(length_stats)} words")
    
    print("üìù Creating submission file...")
    submission_data = []
    try:
        if 'Master_Index' in test_dataset.column_names:
            for i, example in enumerate(test_dataset):
                submission_data.append({
                    'Master_Index': example['Master_Index'],
                    'Clinician': predictions[i]
                })
            print("‚úÖ Using Master_Index from test dataset")
        else:
            for i, pred in enumerate(predictions):
                submission_data.append({
                    'Master_Index': f'ID_{i:08d}',
                    'Clinician': pred
                })
            print("‚ö†Ô∏è Master_Index not found, using generated IDs")
    except Exception as e:
        print(f"‚ùå Error creating submission data: {e}")
        for i, pred in enumerate(predictions):
            submission_data.append({
                'Master_Index': f'ID_{i:08d}',
                'Clinician': pred
            })
    
    submission_df = pd.DataFrame(submission_data)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    submission_df.to_csv(output_path, index=False)
    print(f"‚úÖ Enhanced submission file saved to: {output_path}")
    
    # Save length statistics
    length_stats_file = output_path.replace('.csv', '_length_stats.json')
    with open(length_stats_file, 'w') as f:
        json.dump({
            'average_length': avg_length,
            'min_length': min(length_stats),
            'max_length': max(length_stats),
            'total_predictions': len(length_stats),
            'target_achieved': avg_length >= 75.0,
            'length_distribution': {
                'under_50': sum(1 for l in length_stats if l < 50),
                '50_to_75': sum(1 for l in length_stats if 50 <= l < 75),
                '75_to_100': sum(1 for l in length_stats if 75 <= l < 100),
                'over_100': sum(1 for l in length_stats if l >= 100)
            }
        }, f, indent=2)
    print(f"üìä Length statistics saved to: {length_stats_file}")
    
    return output_path

if __name__ == '__main__':
    try:
        submission_path = run_inference(use_optimized=True)
        print("\nüéâ Enhanced inference completed successfully!")
        print(f"üìÅ Submission file: {submission_path}")
    except Exception as e:
        print(f"‚ùå Enhanced inference failed: {e}")

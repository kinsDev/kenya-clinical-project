from datasets import load_from_disk, Dataset
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import numpy as np
from collections import Counter
import re
import json
import os
import pandas as pd
from pathlib import Path
from sklearn.model_selection import KFold
import time
from typing import List, Dict, Tuple
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def compute_rouge_scores(predictions, references):
    """Compute ROUGE scores with consistent normalization"""
    def normalize_for_evaluation(text):
        if not text or text.strip() == "":
            return "patient requires clinical assessment and appropriate treatment"
        normalized = text.lower()
        normalized = re.sub(r'[^\w\s]', '', normalized)
        normalized = re.sub(r'\s+', ' ', normalized)
        normalized = normalized.strip()
        if not normalized or len(normalized.split()) < 2:
            normalized = "patient requires clinical assessment and appropriate treatment"
        return normalized

    def get_ngrams(text, n):
        text = normalize_for_evaluation(text)
        tokens = text.split()
        return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]

    def rouge_n(pred, ref, n):
        pred_ngrams = get_ngrams(pred, n)
        ref_ngrams = get_ngrams(ref, n)
        if not ref_ngrams:
            return 0.0
        overlap = Counter(pred_ngrams) & Counter(ref_ngrams)
        overlap_count = sum(overlap.values())
        recall = overlap_count / len(ref_ngrams)
        precision = overlap_count / len(pred_ngrams) if pred_ngrams else 0.0
        if precision + recall == 0:
            return 0.0
        f1_score = 2 * precision * recall / (precision + recall)
        return f1_score

    def rouge_l(pred, ref):
        pred_tokens = normalize_for_evaluation(pred).split()
        ref_tokens = normalize_for_evaluation(ref).split()
        m, n = len(pred_tokens), len(ref_tokens)
        if m == 0 or n == 0:
            return 0.0
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if pred_tokens[i-1] == ref_tokens[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        lcs_length = dp[m][n]
        recall = lcs_length / len(ref_tokens)
        precision = lcs_length / len(pred_tokens)
        if precision + recall == 0:
            return 0.0
        f_score = 2 * precision * recall / (precision + recall)
        return f_score

    normalized_predictions = [normalize_for_evaluation(pred) for pred in predictions]
    normalized_references = [normalize_for_evaluation(ref) for ref in references]
    rouge1_scores = [rouge_n(pred, ref, 1) for pred, ref in zip(normalized_predictions, normalized_references)]
    rouge2_scores = [rouge_n(pred, ref, 2) for pred, ref in zip(normalized_predictions, normalized_references)]
    rougeL_scores = [rouge_l(pred, ref) for pred, ref in zip(normalized_predictions, normalized_references)]
    return {
        'rouge1': np.mean(rouge1_scores),
        'rouge2': np.mean(rouge2_scores),
        'rougeL': np.mean(rougeL_scores),
        'rouge1_std': np.std(rouge1_scores),
        'rouge2_std': np.std(rouge2_scores),
        'rougeL_std': np.std(rougeL_scores),
        'individual_scores': {
            'rouge1_scores': rouge1_scores,
            'rouge2_scores': rouge2_scores,
            'rougeL_scores': rougeL_scores
        }
    }

def evaluate_single_fold(model, tokenizer, val_data: List[Dict], device: torch.device, fold_num: int = None) -> Dict:
    """Evaluate model on a single fold of data"""
    predictions = []
    references = []
    sample_ids = []
    prediction_lengths = []
    reference_lengths = []
    
    model.eval()
    fold_prefix = f"[Fold {fold_num}] " if fold_num is not None else ""
    
    with torch.no_grad():
        for i, example in enumerate(val_data):
            if i % 25 == 0:
                print(f"{fold_prefix}Processing sample {i+1}/{len(val_data)}")
            
            try:
                sample_id = example.get('Master_Index', f'VAL_{i:08d}')
                sample_ids.append(sample_id)
                
                # Tokenize input
                inputs = tokenizer(
                    example['Prompt'],
                    return_tensors='pt',
                    truncation=True,
                    padding='max_length',
                    max_length=512
                )
                input_ids = inputs['input_ids'].to(device)
                attention_mask = inputs['attention_mask'].to(device)
                
                # ENHANCED GENERATION PARAMETERS FOR LONGER OUTPUTS
                outputs = model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_length=400,              # Increased from 256
                    min_length=70,               # Minimum length
                    num_beams=6,                 # Increased from 4
                    early_stopping=False,       # Changed - Let it generate more
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    repetition_penalty=1.15,     # Reduced from 1.2
                    length_penalty=1.5,          # Favor longer outputs
                    no_repeat_ngram_size=2,      # Reduced from 3
                )
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                pred_summary = normalize_for_evaluation(generated_text)
                
                # Handle reference text
                ref_text = example.get('Clinician', example.get('labels', 'patient requires clinical assessment'))
                ref_summary = normalize_for_evaluation(ref_text)
                
                predictions.append(pred_summary)
                references.append(ref_summary)
                
                # Track lengths for analysis
                prediction_lengths.append(len(pred_summary.split()))
                reference_lengths.append(len(ref_summary.split()))
                
            except Exception as e:
                logger.error(f"{fold_prefix}Error processing sample {i}: {e}")
                predictions.append(normalize_for_evaluation(""))
                references.append(normalize_for_evaluation(""))
                sample_ids.append(f'ERROR_{i:08d}')
                prediction_lengths.append(0)
                reference_lengths.append(0)

    # Compute metrics
    valid_pairs = [(p, r, sid, pl, rl) for p, r, sid, pl, rl in zip(predictions, references, sample_ids, prediction_lengths, reference_lengths) if p.strip() and r.strip()]
    
    if not valid_pairs:
        logger.error(f"{fold_prefix}No valid prediction-reference pairs found!")
        return None

    valid_predictions, valid_references, valid_ids, valid_pred_lengths, valid_ref_lengths = zip(*valid_pairs)
    
    try:
        rouge_results = compute_rouge_scores(valid_predictions, valid_references)
    except Exception as e:
        logger.error(f"{fold_prefix}Error computing ROUGE: {e}")
        rouge_results = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}

    # Enhanced metrics with length analysis
    avg_pred_length = np.mean(valid_pred_lengths)
    avg_ref_length = np.mean(valid_ref_lengths)
    
    metrics = {
        **rouge_results,
        'num_samples': len(valid_pairs),
        'avg_pred_length': avg_pred_length,
        'avg_ref_length': avg_ref_length,
        'length_ratio': avg_pred_length / avg_ref_length if avg_ref_length > 0 else 0,
        'length_analysis': {
            'pred_length_std': np.std(valid_pred_lengths),
            'ref_length_std': np.std(valid_ref_lengths),
            'min_pred_length': min(valid_pred_lengths),
            'max_pred_length': max(valid_pred_lengths),
            'predictions_over_75_words': sum(1 for l in valid_pred_lengths if l >= 75),
            'predictions_under_50_words': sum(1 for l in valid_pred_lengths if l < 50),
            'length_target_achievement': avg_pred_length >= 75.0
        },
        'format_compliance': {
            'all_lowercase': all(p.islower() for p in valid_predictions),
            'no_punctuation': all(not any(c in p for c in '.,!?;:"()[]{}') for p in valid_predictions),
            'min_word_count': min(valid_pred_lengths),
            'empty_predictions': sum(1 for p in valid_predictions if not p.strip())
        }
    }
    
    return metrics

def cross_validate_model(model_path: str = None, data_path: str = 'data/train.csv', n_splits: int = 5) -> Dict:
    """5-fold cross-validation to check generalization"""
    print("üîÑ Starting 5-Fold Cross-Validation...")
    print("=" * 70)
    print("üéØ CROSS-VALIDATION FEATURES:")
    print("‚úÖ 5-fold stratified validation")
    print("‚úÖ Robust generalization testing")
    print("‚úÖ Statistical significance analysis")
    print("‚úÖ Per-fold detailed metrics")
    print("=" * 70)
    
    model_path = model_path or os.getenv('MODEL_PATH', './model_outputs/final_model')
    
    # Load model and tokenizer
    try:
        model = T5ForConditionalGeneration.from_pretrained(model_path)
        tokenizer = T5Tokenizer.from_pretrained(model_path)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        print(f"‚úÖ Model loaded on {device}")
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return None
    
    # Load full dataset
    try:
        if data_path.endswith('.csv'):
            full_data = pd.read_csv(data_path)
            # Apply same preprocessing as in data_preprocessing.py
            full_data['Prompt'] = full_data['Prompt'].apply(lambda x: f"Clinical scenario: {x}")
            # Normalize Clinician text
            full_data['Clinician'] = full_data['Clinician'].apply(lambda x: normalize_for_evaluation(str(x)))
        else:
            # Assume it's a Hugging Face dataset
            full_dataset = load_from_disk(data_path)
            full_data = full_dataset.to_pandas()
        
        print(f"‚úÖ Loaded {len(full_data)} samples for cross-validation")
    except Exception as e:
        print(f"‚ùå Failed to load data: {e}")
        return None
    
    # Perform K-fold cross-validation
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    fold_scores = []
    fold_metrics = []
    
    start_time = time.time()
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(full_data)):
        print(f"\nüîç Evaluating Fold {fold+1}/{n_splits}...")
        print(f"Validation samples: {len(val_idx)}")
        
        # Get validation data for this fold
        val_data = full_data.iloc[val_idx].to_dict('records')
        
        # Evaluate on this fold
        fold_start = time.time()
        metrics = evaluate_single_fold(model, tokenizer, val_data, device, fold_num=fold+1)
        fold_time = time.time() - fold_start
        
        if metrics:
            fold_scores.append(metrics['rougeL'])
            fold_metrics.append(metrics)
            
            print(f"‚úÖ Fold {fold+1} completed in {fold_time:.1f}s")
            print(f"   ROUGE-L: {metrics['rougeL']:.4f}")
            print(f"   Avg Length: {metrics['avg_pred_length']:.1f} words")
            print(f"   Length Target: {'‚úÖ' if metrics['length_analysis']['length_target_achievement'] else '‚ùå'}")
        else:
            print(f"‚ùå Fold {fold+1} failed")
            fold_scores.append(0.0)
            fold_metrics.append(None)
    
    total_time = time.time() - start_time
    
    # Compute cross-validation statistics
    valid_scores = [score for score in fold_scores if score > 0]
    if not valid_scores:
        print("‚ùå All folds failed!")
        return None
    
    cv_mean = np.mean(valid_scores)
    cv_std = np.std(valid_scores)
    cv_min = np.min(valid_scores)
    cv_max = np.max(valid_scores)
    
    # Aggregate metrics across folds
    valid_metrics = [m for m in fold_metrics if m is not None]
    avg_length_across_folds = np.mean([m['avg_pred_length'] for m in valid_metrics])
    length_consistency = np.std([m['avg_pred_length'] for m in valid_metrics])
    
    print("\n" + "="*70)
    print("üéØ CROSS-VALIDATION RESULTS")
    print("="*70)
    print(f"ROUGE-L Cross-Validation: {cv_mean:.4f} ¬± {cv_std:.4f}")
    print(f"ROUGE-L Range: {cv_min:.4f} - {cv_max:.4f}")
    print(f"Valid Folds: {len(valid_scores)}/{n_splits}")
    print(f"Average Length Across Folds: {avg_length_across_folds:.1f} ¬± {length_consistency:.1f} words")
    print(f"Total CV Time: {total_time/60:.1f} minutes")
    
    # Statistical significance test
    if len(valid_scores) >= 3:
        confidence_interval = 1.96 * cv_std / np.sqrt(len(valid_scores))  # 95% CI
        print(f"95% Confidence Interval: [{cv_mean - confidence_interval:.4f}, {cv_mean + confidence_interval:.4f}]")
        
        # Check for statistical significance (basic threshold test)
        baseline_threshold = 0.3  # Assume baseline ROUGE-L of 0.3
        if cv_mean - confidence_interval > baseline_threshold:
            print(f"‚úÖ Statistically significant improvement over baseline ({baseline_threshold:.3f})")
        else:
            print(f"‚ö†Ô∏è No statistically significant improvement over baseline ({baseline_threshold:.3f})")
    
    # Detailed fold analysis
    print(f"\nüìä PER-FOLD DETAILED ANALYSIS:")
    for i, (score, metrics) in enumerate(zip(fold_scores, fold_metrics)):
        if metrics:
            print(f"Fold {i+1}: ROUGE-L={score:.4f}, Length={metrics['avg_pred_length']:.1f}w, "
                  f"Target={'‚úÖ' if metrics['length_analysis']['length_target_achievement'] else '‚ùå'}")
        else:
            print(f"Fold {i+1}: ‚ùå FAILED")
    
    # Save cross-validation results
    cv_results = {
        'cross_validation_summary': {
            'mean_rouge_l': cv_mean,
            'std_rouge_l': cv_std,
            'min_rouge_l': cv_min,
            'max_rouge_l': cv_max,
            'n_valid_folds': len(valid_scores),
            'total_folds': n_splits,
            'avg_length_across_folds': avg_length_across_folds,
            'length_consistency_std': length_consistency,
            'total_time_minutes': total_time / 60,
            'confidence_interval_95': confidence_interval if len(valid_scores) >= 3 else None
        },
        'fold_details': [
            {
                'fold': i+1,
                'rouge_l': score,
                'metrics': metrics,
                'status': 'success' if metrics else 'failed'
            }
            for i, (score, metrics) in enumerate(zip(fold_scores, fold_metrics))
        ],
        'statistical_analysis': {
            'baseline_threshold': baseline_threshold,
            'significant_improvement': cv_mean - (confidence_interval if len(valid_scores) >= 3 else 0) > baseline_threshold,
            'model_stability': 'high' if cv_std < 0.05 else 'medium' if cv_std < 0.1 else 'low'
        }
    }
    
    # Save results
    results_file = Path('./experiments/cross_validation_results.json')
    results_file.parent.mkdir(exist_ok=True)
    with open(results_file, 'w') as f:
        json.dump(cv_results, f, indent=2, default=str)
    print(f"üíæ Cross-validation results saved to: {results_file}")
    
    return cv_results

def evaluate_model(model_path=None, val_path=None, include_cross_validation=False):
    """Evaluate model with enhanced generation parameters for longer predictions"""
    model_path = model_path or os.getenv('MODEL_PATH', './model_outputs/final_model')
    val_path = val_path or os.getenv('VAL_PATH', 'outputs/val_dataset')

    print("üîç Starting enhanced model evaluation with longer generation...")
    print("üîß ENHANCED EVALUATION FEATURES:")
    print("‚úÖ Enhanced generation parameters for longer outputs")
    print("‚úÖ Length-aware evaluation metrics")
    print("‚úÖ Detailed length distribution analysis")
    print("‚úÖ Cross-validation support")
    print("‚úÖ Statistical significance testing")
    print("=" * 60)
    print(f"Model path: {model_path}")
    print(f"Validation dataset path: {val_path}")

    if not Path(model_path).exists():
        print(f"‚ùå Model path does not exist: {model_path}")
        return None
    if not Path(val_path).exists():
        print(f"‚ùå Validation dataset path does not exist: {val_path}")
        return None

    try:
        model = T5ForConditionalGeneration.from_pretrained(model_path)
        tokenizer = T5Tokenizer.from_pretrained(model_path)
        val_dataset = load_from_disk(val_path)
        print(f"‚úÖ Loaded model and {len(val_dataset)} validation samples")
        print(f"Dataset columns: {val_dataset.column_names}")
    except Exception as e:
        print(f"‚ùå Error loading model or data: {e}")
        return None

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    
    # Convert dataset to list of dicts for consistency
    val_data = []
    for example in val_dataset:
        val_data.append(example)
    
    print(f"üöÄ Evaluating on {len(val_data)} samples using {device}...")
    
    # Perform single evaluation
    metrics = evaluate_single_fold(model, tokenizer, val_data, device)
    
    if not metrics:
        print("‚ùå Evaluation failed!")
        return None

    print("\n" + "="*70)
    print("üéØ ENHANCED EVALUATION RESULTS")
    print("="*70)
    print(f"ROUGE-1: {metrics['rouge1']:.4f} (¬±{metrics.get('rouge1_std', 0):.4f})")
    print(f"ROUGE-2: {metrics['rouge2']:.4f} (¬±{metrics.get('rouge2_std', 0):.4f})")
    print(f"ROUGE-L: {metrics['rougeL']:.4f} (¬±{metrics.get('rougeL_std', 0):.4f})")
    
    print(f"\nüìè LENGTH ANALYSIS (KEY IMPROVEMENT METRIC):")
    print(f"Average Prediction Length: {metrics['avg_pred_length']:.1f} words")
    print(f"Average Reference Length: {metrics['avg_ref_length']:.1f} words")
    print(f"Length Ratio (Pred/Ref): {metrics['length_ratio']:.2f}")
    print(f"Target Achievement (‚â•75 words): {'‚úÖ YES' if metrics['length_analysis']['length_target_achievement'] else '‚ùå NO'}")
    print(f"Predictions ‚â•75 words: {metrics['length_analysis']['predictions_over_75_words']}/{metrics['num_samples']} ({100*metrics['length_analysis']['predictions_over_75_words']/metrics['num_samples']:.1f}%)")
    print(f"Predictions <50 words: {metrics['length_analysis']['predictions_under_50_words']}/{metrics['num_samples']} ({100*metrics['length_analysis']['predictions_under_50_words']/metrics['num_samples']:.1f}%)")
    print(f"Length Range: {metrics['length_analysis']['min_pred_length']} - {metrics['length_analysis']['max_pred_length']} words")
    
    print(f"\nüìà Format Compliance:")
    print(f"All lowercase: {'‚úÖ' if metrics['format_compliance']['all_lowercase'] else '‚ùå'}")
    print(f"No punctuation: {'‚úÖ' if metrics['format_compliance']['no_punctuation'] else '‚ùå'}")
    print(f"Min word count: {metrics['format_compliance']['min_word_count']}")
    print(f"Empty predictions: {metrics['format_compliance']['empty_predictions']}")
    
    print(f"\nüìà Statistics:")
    print(f"Samples Evaluated: {metrics['num_samples']}")

    # Save detailed results
    results_file = Path('./experiments/enhanced_evaluation_results.json')
    results_file.parent.mkdir(exist_ok=True)
    detailed_results = {
        'metrics': metrics,
        'evaluation_enhancements': {
            'generation_config': {
                'max_length': 400,
                'min_length': 70,
                'num_beams': 6,
                'length_penalty': 1.5,
                'early_stopping': False
            },
            'target_length_achieved': metrics['length_analysis']['length_target_achievement'],
            'length_improvement_vs_baseline': f"Target: >75 words, Achieved: {metrics['avg_pred_length']:.1f} words"
        },
        'format_verification': {
            'normalization_applied': True,
            'single_task_focus': True,
            'consistent_with_inference': True,
            'enhanced_evaluation': True
        },
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open(results_file, 'w') as f:
        json.dump(detailed_results, f, indent=2, default=str)
    print(f"üíæ Enhanced results saved to: {results_file}")

    # Perform cross-validation if requested
    cv_results = None
    if include_cross_validation:
        print(f"\nüîÑ Starting Cross-Validation...")
        cv_results = cross_validate_model(model_path)
        if cv_results:
            print(f"‚úÖ Cross-validation completed!")
            print(f"CV ROUGE-L: {cv_results['cross_validation_summary']['mean_rouge_l']:.4f} ¬± {cv_results['cross_validation_summary']['std_rouge_l']:.4f}")
        else:
            print(f"‚ùå Cross-validation failed!")

    # Combine results
    final_results = {
        'single_evaluation': metrics,
        'cross_validation': cv_results,
        'evaluation_summary': {
            'primary_rouge_l': metrics['rougeL'],
            'cv_rouge_l': cv_results['cross_validation_summary']['mean_rouge_l'] if cv_results else None,
            'length_target_achieved': metrics['length_analysis']['length_target_achievement'],
            'model_stability': cv_results['statistical_analysis']['model_stability'] if cv_results else 'unknown',
            'recommendation': 'APPROVED' if metrics['rougeL'] > 0.4 and metrics['length_analysis']['length_target_achievement'] else 'NEEDS_IMPROVEMENT'
        }
    }

    return final_results

def normalize_for_evaluation(text):
    """Helper function for normalization"""
    if not text or text.strip() == "":
        return "patient requires clinical assessment and appropriate treatment"
    normalized = text.lower()
    normalized = re.sub(r'[^\w\s]', '', normalized)
    normalized = re.sub(r'\s+', ' ', normalized)
    normalized = normalized.strip()
    if not normalized or len(normalized.split()) < 2:
        normalized = "patient requires clinical assessment and appropriate treatment"
    return normalized

def run_comprehensive_evaluation(model_path: str = None, include_cv: bool = True):
    """Run comprehensive evaluation with all enhancements"""
    print("üöÄ Starting Comprehensive Model Evaluation...")
    print("=" * 70)
    print("üîß COMPREHENSIVE EVALUATION FEATURES:")
    print("‚úÖ Enhanced generation parameters for longer outputs")
    print("‚úÖ Length-focused evaluation metrics")
    print("‚úÖ 5-fold cross-validation")
    print("‚úÖ Statistical significance testing")
    print("‚úÖ Model stability analysis")
    print("‚úÖ Detailed performance breakdown")
    print("=" * 70)
    
    try:
        results = evaluate_model(
            model_path=model_path,
            include_cross_validation=include_cv
        )
        
        if results:
            print(f"\nüéØ COMPREHENSIVE EVALUATION SUMMARY:")
            print(f"Primary ROUGE-L: {results['evaluation_summary']['primary_rouge_l']:.4f}")
            if results['evaluation_summary']['cv_rouge_l']:
                print(f"Cross-Validation ROUGE-L: {results['evaluation_summary']['cv_rouge_l']:.4f}")
            print(f"Length Target: {'‚úÖ ACHIEVED' if results['evaluation_summary']['length_target_achieved'] else '‚ùå NOT ACHIEVED'}")
            print(f"Model Stability: {results['evaluation_summary']['model_stability'].upper()}")
            print(f"Overall Recommendation: {results['evaluation_summary']['recommendation']}")
            
            # Save comprehensive results
            comprehensive_file = Path('./experiments/comprehensive_evaluation_results.json')
            with open(comprehensive_file, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            print(f"üíæ Comprehensive results saved to: {comprehensive_file}")
            
            return results
        else:
            print("‚ùå Comprehensive evaluation failed: No results returned")
            return None
            
    except Exception as e:
        print(f"‚ùå Comprehensive evaluation failed: {e}")
        logger.error(f"Comprehensive evaluation error: {e}")
        return None

if __name__ == '__main__':
    try:
        # Check for command line arguments
        import sys
        include_cv = '--cross-validation' in sys.argv or '-cv' in sys.argv
        model_path = None
        
        # Parse model path if provided
        for i, arg in enumerate(sys.argv):
            if arg == '--model-path' and i + 1 < len(sys.argv):
                model_path = sys.argv[i + 1]
                break
        
        print("üöÄ Starting Enhanced Model Evaluation...")
        print("=" * 70)
        print("üîß EVALUATION ENHANCEMENTS:")
        print("‚úÖ Enhanced generation parameters for longer outputs")
        print("‚úÖ Length-focused evaluation metrics")
        print("‚úÖ Target: Average prediction length ‚â•75 words")
        if include_cv:
            print("‚úÖ 5-fold cross-validation enabled")
        print("=" * 70)
        
        if include_cv:
            results = run_comprehensive_evaluation(model_path=model_path, include_cv=True)
        else:
            results = evaluate_model(model_path=model_path, include_cross_validation=False)
        
        if results:
            if isinstance(results, dict) and 'evaluation_summary' in results:
                # Comprehensive results
                print(f"\nüìä Final Comprehensive Results:")
                print(f"Primary ROUGE-L: {results['evaluation_summary']['primary_rouge_l']:.4f}")
                if results['evaluation_summary']['cv_rouge_l']:
                    print(f"CV ROUGE-L: {results['evaluation_summary']['cv_rouge_l']:.4f}")
                print(f"Length Target: {'‚úÖ ACHIEVED' if results['evaluation_summary']['length_target_achieved'] else '‚ùå NOT ACHIEVED'}")
                print(f"Recommendation: {results['evaluation_summary']['recommendation']}")
            else:
                # Single evaluation results
                print(f"\nüìä Final Results:")
                print(f"ROUGE-L: {results['rougeL']:.4f}")
                print(f"Average Length: {results['avg_pred_length']:.1f} words")
                print(f"Length Target: {'‚úÖ ACHIEVED' if results['length_analysis']['length_target_achievement'] else '‚ùå NOT ACHIEVED'}")
        else:
            print("‚ùå Evaluation failed: No metrics returned")
            
    except Exception as e:
        print(f"‚ùå Enhanced evaluation failed: {e}")
        logger.error(f"Main evaluation error: {e}")

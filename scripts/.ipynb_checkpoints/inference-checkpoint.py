import pandas as pd
from datasets import load_from_disk
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
from pathlib import Path
import os
import json
import re
import time
from typing import List, Dict

class InferenceEngine:
    """Inference engine for generating summaries"""
    def __init__(self, model_path: str, use_optimized: bool = True):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self._load_model(use_optimized)

    def _load_model(self, use_optimized: bool):
        """Load the best available model version"""
        print("üîç Searching for best available model...")
        model_candidates = []
        if use_optimized:
            base_path = Path(self.model_path).parent
            optimized_candidates = [
                (base_path / 'optimized_model' / 'fp16', 'FP16 Optimized'),
                (base_path / 'optimized_model' / 'quantized', 'Quantized'),
            ]
            for path, name in optimized_candidates:
                if path.exists() and any(path.iterdir()):
                    model_candidates.append((str(path), name))
        model_candidates.append((self.model_path, 'Original'))
        
        for model_path, model_name in model_candidates:
            try:
                print(f"üîÑ Trying to load {model_name} model from: {model_path}")
                self.model = T5ForConditionalGeneration.from_pretrained(model_path)
                self.tokenizer = T5Tokenizer.from_pretrained(model_path)
                self.model.to(self.device)
                self.model.eval()
                print(f"‚úÖ Successfully loaded {model_name} model")
                return
            except Exception as e:
                print(f"‚ùå Failed to load {model_name}: {e}")
                continue
        raise RuntimeError("‚ùå Could not load any model version!")

    def _normalize_for_submission(self, summary: str) -> str:
        """Apply ONLY the normalization needed for submission format"""
        if not summary or summary.strip() == "":
            return "patient requires clinical assessment and appropriate treatment based on presenting symptoms and medical history"
        
        # Keep original case and punctuation - only clean up spacing
        normalized = re.sub(r'\s+', ' ', summary.strip())
        
        # Only convert to lowercase if that's the submission requirement
        normalized = normalized.lower()
        
        # Remove punctuation only if required by submission format
        normalized = re.sub(r'[^\w\s]', '', normalized)
        
        if not normalized or len(normalized.split()) < 2:
            normalized = "patient requires clinical assessment and appropriate treatment based on presenting symptoms and medical history"
        
        return normalized

    def generate_summary(self, prompt: str, generation_config: Dict = None) -> str:
        """Generate with more conservative parameters"""
        if generation_config is None:
            # More conservative generation config
            generation_config = {
                'max_length': 300,           # Reduced from 400
                'min_length': 50,            # Reduced from 70
                'num_beams': 4,              # Reduced from 6 for speed
                'early_stopping': True,      # Changed back to True
                'do_sample': False,          # Keep deterministic
                'repetition_penalty': 1.1,   # Reduced from 1.15
                'length_penalty': 1.2,       # Reduced from 1.5
                'no_repeat_ngram_size': 3,   # Increased back to 3
                'forced_bos_token_id': None,
                'forced_eos_token_id': None,
                'exponential_decay_length_penalty': None,
                'suppress_tokens': None,
                'begin_suppress_tokens': None,
            }
        
        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors='pt',
                truncation=True,
                padding='max_length',
                max_length=512
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            print(f"üîß Using conservative generation config: max_len={generation_config['max_length']}, min_len={generation_config['min_length']}, beams={generation_config['num_beams']}, length_penalty={generation_config['length_penalty']}")
            
            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs['input_ids'],
                    attention_mask=inputs['attention_mask'],
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    **generation_config
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            normalized_summary = self._normalize_for_submission(generated_text)
            
            # Log length for monitoring
            word_count = len(normalized_summary.split())
            if word_count < 30:
                print(f"‚ö†Ô∏è Short prediction ({word_count} words): {normalized_summary[:100]}...")
            elif word_count > 60:
                print(f"‚úÖ Good length prediction ({word_count} words)")
            
            return normalized_summary
            
        except Exception as e:
            print(f"‚ùå Failed to generate sample for prompt: {prompt[:50]}... Error: {e}")
            return self._normalize_for_submission("patient requires clinical assessment and appropriate treatment based on presenting symptoms and medical history")

def run_inference(
        model_path='./model_outputs/final_model',
        test_path='outputs/test_dataset',
        output_path='outputs/submission.csv',
        use_optimized=True
):
    """Run inference with improved generation parameters"""
    print("üöÄ Starting Improved Inference with Conservative Generation...")
    print("=" * 70)
    print("üîß IMPROVEMENTS APPLIED:")
    print("‚úÖ Conservative generation parameters for stability")
    print("‚úÖ Improved normalization for submission format")
    print("‚úÖ Better error handling and fallbacks")
    print("‚úÖ Optimized length targets (50-300 words)")
    print("‚úÖ Enhanced monitoring and logging")
    print("=" * 70)
    
    try:
        inference_engine = InferenceEngine(model_path, use_optimized)
        test_dataset = load_from_disk(test_path)
        print(f"‚úÖ Loaded {len(test_dataset)} test samples")
    except Exception as e:
        print(f"‚ùå Failed to initialize inference engine: {e}")
        raise
    
    print(f"üéØ Running improved inference on {len(test_dataset)} samples...")
    print(f"üíª Using device: {inference_engine.device}")
    
    predictions = []
    length_stats = []
    start_time = time.time()
    
    for i, example in enumerate(test_dataset):
        if i % 25 == 0:
            elapsed = time.time() - start_time
            if i > 0:
                eta = (elapsed / i) * (len(test_dataset) - i)
                avg_length = sum(length_stats) / len(length_stats) if length_stats else 0
                print(f"üìä Progress: {i+1}/{len(test_dataset)} ({100*(i+1)/len(test_dataset):.1f}%) - ETA: {eta/60:.1f}min - Avg Length: {avg_length:.1f} words")
        
        try:
            summary = inference_engine.generate_summary(example['Prompt'])
            predictions.append(summary)
            length_stats.append(len(summary.split()))
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing sample {i}: {e}")
            fallback = inference_engine._normalize_for_submission("patient requires clinical assessment and appropriate treatment based on presenting symptoms and medical history")
            predictions.append(fallback)
            length_stats.append(len(fallback.split()))
    
    total_time = time.time() - start_time
    avg_length = sum(length_stats) / len(length_stats)
    
    print(f"‚úÖ Improved inference completed in {total_time/60:.1f} minutes")
    print(f"‚ö° Average time per sample: {total_time/len(test_dataset):.3f}s")
    print(f"üìè AVERAGE LENGTH: {avg_length:.1f} words (Target: 50-80 words)")
    print(f"üìè Length range: {min(length_stats)} - {max(length_stats)} words")
    
    # Length distribution analysis
    short_predictions = sum(1 for l in length_stats if l < 40)
    good_predictions = sum(1 for l in length_stats if 40 <= l <= 100)
    long_predictions = sum(1 for l in length_stats if l > 100)
    
    print(f"üìä Length Distribution:")
    print(f"  Short (<40 words): {short_predictions} ({100*short_predictions/len(length_stats):.1f}%)")
    print(f"  Good (40-100 words): {good_predictions} ({100*good_predictions/len(length_stats):.1f}%)")
    print(f"  Long (>100 words): {long_predictions} ({100*long_predictions/len(length_stats):.1f}%)")
    
    print("üìù Creating submission file...")
    submission_data = []
    try:
        if 'Master_Index' in test_dataset.column_names:
            for i, example in enumerate(test_dataset):
                submission_data.append({
                    'Master_Index': example['Master_Index'],
                    'Clinician': predictions[i]
                })
            print("‚úÖ Using Master_Index from test dataset")
        else:
            for i, pred in enumerate(predictions):
                submission_data.append({
                    'Master_Index': f'ID_{i:08d}',
                    'Clinician': pred
                })
            print("‚ö†Ô∏è Master_Index not found, using generated IDs")
    except Exception as e:
        print(f"‚ùå Error creating submission data: {e}")
        for i, pred in enumerate(predictions):
            submission_data.append({
                'Master_Index': f'ID_{i:08d}',
                'Clinician': pred
            })
    
    submission_df = pd.DataFrame(submission_data)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    submission_df.to_csv(output_path, index=False)
    print(f"‚úÖ Improved submission file saved to: {output_path}")
    
    # Save detailed statistics
    stats_file = output_path.replace('.csv', '_stats.json')
    with open(stats_file, 'w') as f:
        json.dump({
            'inference_stats': {
                'total_samples': len(predictions),
                'average_length': avg_length,
                'min_length': min(length_stats),
                'max_length': max(length_stats),
                'total_time_minutes': total_time / 60,
                'avg_time_per_sample': total_time / len(test_dataset),
                'target_range_achieved': 50 <= avg_length <= 80
            },
            'length_distribution': {
                'short_predictions': short_predictions,
                'good_predictions': good_predictions,
                'long_predictions': long_predictions,
                'short_percentage': 100 * short_predictions / len(length_stats),
                'good_percentage': 100 * good_predictions / len(length_stats),
                'long_percentage': 100 * long_predictions / len(length_stats)
            },
            'generation_config': {
                'max_length': 300,
                'min_length': 50,
                'num_beams': 4,
                'early_stopping': True,
                'length_penalty': 1.2,
                'repetition_penalty': 1.1
            },
            'improvements_applied': [
                'Conservative generation parameters',
                'Improved normalization',
                'Better error handling',
                'Enhanced monitoring'
            ]
        }, f, indent=2)
    print(f"üìä Detailed statistics saved to: {stats_file}")
    
    return output_path

if __name__ == '__main__':
    try:
        submission_path = run_inference(use_optimized=True)
        print("\nüéâ Improved inference completed successfully!")
        print(f"üìÅ Submission file: {submission_path}")
        print("\nüîß Key Improvements:")
        print("‚úÖ More conservative generation parameters")
        print("‚úÖ Better submission format normalization")
        print("‚úÖ Enhanced error handling and fallbacks")
        print("‚úÖ Improved length targeting (50-80 words)")
        print("‚úÖ Detailed statistics and monitoring")
    except Exception as e:
        print(f"‚ùå Improved inference failed: {e}")
        import traceback
        traceback.print_exc()

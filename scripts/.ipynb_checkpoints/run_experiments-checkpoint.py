import subprocess
import os
import time
import json
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import re

class ExperimentRunner:
    def __init__(self, max_parallel=2):
        self.max_parallel = max_parallel
        self.results = {}
        self.lock = threading.Lock()

    def check_config_exists(self, config_name):
        """Verify that the configuration file exists"""
        config_path = Path(f"conf/experiments/{config_name}.yaml")
        if not config_path.exists():
            print(f"âŒ Configuration file not found: {config_path}")
            return False
        return True

    def run_single_experiment(self, config_name, experiment_name):
        print(f"\nðŸš€ Starting Experiment: {experiment_name}")
        print(f"Config: experiments/{config_name}")
        print(f"Time: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print("-" * 50)

        if not self.check_config_exists(config_name):
            raise FileNotFoundError(f"Configuration experiments/{config_name}.yaml not found")

        start_time = time.time()
        exp_dir = Path(f"./experiments/{experiment_name}")
        exp_dir.mkdir(parents=True, exist_ok=True)

        try:
            cmd = [
                'python', 'scripts/model_training.py',
                f'--config-path=../conf/experiments',
                f'--config-name={config_name}',
                f'hydra.run.dir={exp_dir}/hydra_outputs'
            ]
            print(f"ðŸ”§ Running: {' '.join(cmd)}")
            env = os.environ.copy()
            env['HYDRA_FULL_ERROR'] = '1'  # Enable full stack traces
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd(), env=env, check=True)
            training_time = time.time() - start_time

            print(f"âœ… Training completed for {experiment_name} ({training_time:.1f}s)")

            eval_start = time.time()
            eval_cmd = ['python', 'scripts/model_evaluation.py']
            env['MODEL_PATH'] = str(exp_dir / 'final_model')
            env['VAL_PATH'] = 'outputs/val_dataset'
            eval_result = subprocess.run(eval_cmd, capture_output=True, text=True, env=env)
            eval_time = time.time() - eval_start

            metrics = self.parse_evaluation_output(eval_result.stdout)
            if not metrics:
                print(f"âš ï¸ No metrics parsed from evaluation output for {experiment_name}")

            experiment_result = {
                'status': 'success' if eval_result.returncode == 0 else 'failed',
                'training_time': training_time,
                'evaluation_time': eval_time,
                'total_time': training_time + eval_time,
                'metrics': metrics if metrics else {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0},
                'config_name': config_name,
                'output_dir': str(exp_dir),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'scheduler_type': 'ReduceLROnPlateau',  # NEW: Track scheduler type
                'experimentation': 1,  # NEW: Track experimentation phase
                'eval_stderr': eval_result.stderr[-500:] if eval_result.returncode != 0 else ''
            }

            print(f"âœ… {experiment_name} COMPLETED! Time: {training_time:.1f}s")
            if metrics:
                print(f"ðŸ“Š ROUGE-L: {metrics.get('rougeL', 'N/A'):.4f}")

        except subprocess.CalledProcessError as e:
            experiment_result = {
                'status': 'failed',
                'error': e.stderr[-1000:],
                'training_time': time.time() - start_time,
                'config_name': config_name,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'scheduler_type': 'ReduceLROnPlateau',
                'experimentation': 1
            }
            print(f"âŒ {experiment_name} FAILED! ({experiment_result['training_time']:.1f}s)")
            print(f"Error: {e.stderr[-500:]}")

        except Exception as e:
            experiment_result = {
                'status': 'error',
                'error': str(e),
                'config_name': config_name,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'scheduler_type': 'ReduceLROnPlateau',
                'experimentation': 1
            }
            print(f"ðŸ’¥ Exception in {experiment_name}: {e}")

        with self.lock:
            self.results[experiment_name] = experiment_result
        return experiment_result

    def parse_evaluation_output(self, output):
        """Parse evaluation metrics with robust error handling"""
        metrics = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}
        if not output or not isinstance(output, str):
            print("âš ï¸ Evaluation output is empty or invalid")
            return metrics

        lines = output.split('\n')
        for line in lines:
            line = line.strip()
            try:
                if 'ROUGE-1:' in line:
                    score = float(re.search(r'ROUGE-1:\s*([\d.]+)', line).group(1))
                    metrics['rouge1'] = score
                elif 'ROUGE-2:' in line:
                    score = float(re.search(r'ROUGE-2:\s*([\d.]+)', line).group(1))
                    metrics['rouge2'] = score
                elif 'ROUGE-L:' in line:
                    score = float(re.search(r'ROUGE-L:\s*([\d.]+)', line).group(1))
                    metrics['rougeL'] = score
            except (AttributeError, ValueError) as e:
                print(f"âš ï¸ Failed to parse metric line: {line} ({e})")
        return metrics

    def run_experiment_batch(self, experiments, batch_name):
        print(f"ðŸŽ¯ Running Batch {batch_name}: {len(experiments)} experiments")
        print(f"ðŸš€ Experiments in this batch:")
        for config, name in experiments:
            print(f"   - {name} ({config})")
        print("=" * 80)

        start_time = time.time()
        with ThreadPoolExecutor(max_workers=self.max_parallel) as executor:
            future_to_exp = {
                executor.submit(self.run_single_experiment, config, name): name
                for config, name in experiments
            }
            completed = 0
            for future in as_completed(future_to_exp):
                exp_name = future_to_exp[future]
                completed += 1
                try:
                    result = future.result()
                    status_emoji = "âœ…" if result['status'] == 'success' else "âŒ"
                    scheduler_info = f" [ReduceLROnPlateau]" if result.get('scheduler_type') == 'ReduceLROnPlateau' else ""
                    print(f"\n{status_emoji} [{completed}/{len(experiments)}] {exp_name}{scheduler_info}: {result['status']}")
                    if result.get('metrics'):
                        print(f"   ðŸ“Š ROUGE-L: {result['metrics'].get('rougeL', 0):.4f}")
                    else:
                        print(f"   âš ï¸ No metrics available for {exp_name}")
                except Exception as e:
                    print(f"ðŸ’¥ {exp_name} crashed: {e}")

        batch_time = time.time() - start_time
        print(f"\nðŸ Batch {batch_name} completed in {batch_time:.1f}s")
        return self.results

# Updated experiment batches with new adaptive configs
EXPERIMENT_BATCHES = {
    1: [
        ("baseline", "baseline"),
        ("quality", "quality")
    ],
    2: [
        ("enhanced", "enhanced"),
        ("optimized", "optimized")
    ],
    3: [
        ("baseline_v2", "baseline_v2"),
        ("optimized_v2", "optimized_v2")
    ],
    4: [
        ("optimized_adaptive", "optimized_adaptive"),          
        ("baseline_adaptive", "baseline_adaptive")      
    ],
    5: [
        ("optimized_enhanced", "optimized_enhanced"),          
        ("baseline_enhanced", "baseline_enhanced") 
    ],
    6: [
        ("length_optimized", "length_optimized")
    ]
}

def run_batch_experiments(batch_number=1):
    if batch_number not in EXPERIMENT_BATCHES:
        print(f"âŒ Invalid batch number: {batch_number}")
        print(f"Available batches: {list(EXPERIMENT_BATCHES.keys())}")
        return

    experiments = EXPERIMENT_BATCHES[batch_number]
    print("ðŸŽ¯ EXPERIMENTATION 1: ReduceLROnPlateau SCHEDULER")
    print("=" * 70)
    print("ðŸ”§ NEW FEATURES IN THIS EXPERIMENTATION:")
    print("âœ… ReduceLROnPlateau adaptive learning rate scheduler")
    print("âœ… Configurable patience, factor, and threshold parameters")
    print("âœ… Enhanced monitoring and logging of LR changes")
    print("âœ… Maintains all existing optimizations")
    print("âœ… New adaptive configurations for testing")
    print("âœ… Fixed config path references for proper Hydra loading")
    if batch_number >= 3:
        print("ðŸ†• RUNNING EXPERIMENTATION 1 CONFIGS!")
    print("=" * 70)

    runner = ExperimentRunner(max_parallel=2)
    results = runner.run_experiment_batch(experiments, batch_number)
    if results:
        try:
            best_exp_name = max(results, key=lambda x: results[x].get('metrics', {}).get('rougeL', 0))
            best_result = results[best_exp_name]
            print(f"\nðŸ† BATCH {batch_number} WINNER: {best_exp_name}")
            if best_result.get('metrics'):
                print(f"ðŸ“ˆ ROUGE-L Score: {best_result['metrics'].get('rougeL', 0):.4f}")
                print(f"ðŸ“ Model Location: {best_result['output_dir']}/final_model")
                print(f"âš™ï¸ Config Used: conf/experiments/{best_result['config_name']}.yaml")
                print(f"â±ï¸ Training Time: {best_result['total_time']/60:.1f} minutes")
                print(f"ðŸ”§ Scheduler: {best_result.get('scheduler_type', 'Unknown')}")
                print(f"ðŸ§ª Experimentation: {best_result.get('experimentation', 'N/A')}")
            else:
                print(f"âš ï¸ No metrics available for winner {best_exp_name}")
        except Exception as e:
            print(f"âŒ Failed to determine batch winner: {e}")
    return results

if __name__ == "__main__":
    import sys
    batch_number = int(sys.argv[1]) if len(sys.argv) > 1 else 4  # Default to new adaptive batch
    results = run_batch_experiments(batch_number)

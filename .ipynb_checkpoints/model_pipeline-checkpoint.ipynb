{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda042f5-158e-4e8b-90c8-a2a52d4a956a",
   "metadata": {},
   "source": [
    "# Kenya Medical Vignettes Model Pipeline\n",
    "\n",
    "## This notebook orchestrates the ML pipeline for predicting clinician responses to vignettes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928d6a3-7706-47ad-8bbb-3666d30c52cb",
   "metadata": {},
   "source": [
    "## 1. Cell 1: Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d56148-a55b-41be-b73c-2e0ed3615084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess \n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Install dependencies from requirements.txt\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'])\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from datasets import load_from_disk \n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3254db-364f-4234-806c-cd0dff08c384",
   "metadata": {},
   "source": [
    "## 2. Cell 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcd766-cafd-4025-a26f-b568e67e9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we're in the project root directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Verify the data files exist\n",
    "print(\"Train file exists:\", os.path.exists('data/train.csv'))\n",
    "print(\"Test file exists:\", os.path.exists('data/test.csv'))\n",
    "\n",
    "print(\"\\n🚀 PRIORITY FIXES: ENHANCED DATA PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"🔧 PRIORITY FIXES APPLIED:\")\n",
    "print(\"✅ Removed multitask learning components\")\n",
    "print(\"✅ Simplified prompt format\")\n",
    "print(\"✅ Removed few-shot examples\")\n",
    "print(\"✅ Implemented basic augmentation (synonym replacement and noise injection)\")\n",
    "print(\"✅ Consistent tokenizer handling with default t5-small\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run the updated data preprocessing script\n",
    "result = subprocess.run(['python', 'scripts/data_preprocessing.py'],\n",
    "                        capture_output=True, text=True, cwd=os.getcwd())\n",
    "\n",
    "print(\"Return code:\", result.returncode)\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Only proceed if the script ran successfully\n",
    "if result.returncode == 0:\n",
    "    # Load processed datasets\n",
    "    train_dataset = load_from_disk('outputs/train_dataset')\n",
    "    val_dataset = load_from_disk('outputs/val_dataset')\n",
    "    test_dataset = load_from_disk('outputs/test_dataset')\n",
    "\n",
    "    print(f\"\\n📊 Dataset Sizes:\")\n",
    "    print(f'Train size: {len(train_dataset)} (with basic augmentation)')\n",
    "    print(f'Validation size: {len(val_dataset)}')\n",
    "    print(f'Test size: {len(test_dataset)}')\n",
    "\n",
    "    # Show sample of enhanced features\n",
    "    print(f\"\\n🔍 Sample Verification:\")\n",
    "    print(\"Sample train example:\")\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Prompt length: {len(sample['Prompt'])} chars\")\n",
    "    print(f\"Target length: {len(sample['Clinician'])} chars\" if 'Clinician' in sample else \"No target (test data)\")\n",
    "\n",
    "    # Verify augmentation\n",
    "    print(f\"\\n🔄 Augmentation Verification:\")\n",
    "    original_prompts = [ex['Prompt'] for ex in train_dataset if 'original' in ex.get('augmentation_type', '')]\n",
    "    augmented_prompts = [ex['Prompt'] for ex in train_dataset if 'augmented' in ex.get('augmentation_type', '')]\n",
    "    print(f\"Original prompts: {len(original_prompts)}\")\n",
    "    print(f\"Augmented prompts: {len(augmented_prompts)}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 Preprocessing Completed Successfully!\")\n",
    "    print(\"Ready for training with simplified, consistent format\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"❌ Preprocessing failed! Check error messages above.\")\n",
    "    print(\"Cannot proceed to training without successful preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ba4ac-e7ee-4c75-a1dc-3e7316201ad1",
   "metadata": {},
   "source": [
    "## 3. Cell 3: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f98d5-2f77-4853-ac69-a6546e7c21d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CURRENT_BATCH = 4\n",
    "\n",
    "EXPERIMENT_BATCHES = {\n",
    "    1: [(\"baseline\", \"baseline\"), (\"fast\", \"fast_training\")],\n",
    "    2: [(\"aggressive\", \"aggressive\"), (\"data_augmented\", \"data_augmented\")],\n",
    "    3: [(\"balanced\", \"balanced\"), (\"quality\", \"quality\")],\n",
    "    4: [(\"enhanced\", \"enhanced\"), (\"optimized\", \"optimized\")]\n",
    "}\n",
    "\n",
    "# DEBUG: Print what will actually run\n",
    "print(\"🔍 DEBUG: Current batch configuration:\")\n",
    "for config, name in EXPERIMENT_BATCHES[CURRENT_BATCH]:\n",
    "    print(f\"  Config: {config}, Experiment Name: {name}\")\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Verify environment before running experiments\"\"\"\n",
    "    required_paths = [\n",
    "        'outputs/train_dataset',\n",
    "        'outputs/val_dataset',\n",
    "        'scripts/model_training.py',\n",
    "        'scripts/run_experiments.py',\n",
    "        'conf/config.yaml'\n",
    "    ]\n",
    "    for path in required_paths:\n",
    "        if not Path(path).exists():\n",
    "            print(f\"❌ Missing required path: {path}\")\n",
    "            return False\n",
    "    for config, name in EXPERIMENT_BATCHES[CURRENT_BATCH]:\n",
    "        config_path = f\"conf/experiments/{config}.yaml\"\n",
    "        if not Path(config_path).exists():\n",
    "            print(f\"❌ Missing configuration file: {config_path}\")\n",
    "            return False\n",
    "    print(\"✅ Environment check passed\")\n",
    "    return True\n",
    "\n",
    "def monitor_training_realtime(experiments, process):\n",
    "    \"\"\"Monitor training progress in real-time using trainer_state.json files\"\"\"\n",
    "    training_data = {exp_name: {'loss': [], 'steps': [], 'eval_loss': [], 'eval_steps': []} for _, exp_name in experiments}\n",
    "    \n",
    "    def update_data():\n",
    "        for config_name, exp_name in experiments:\n",
    "            # Look for the latest checkpoint in the actual training directory\n",
    "            training_dir = Path(f\"./experiments/{config_name}/training\")\n",
    "            \n",
    "            if training_dir.exists():\n",
    "                # Find the latest checkpoint\n",
    "                checkpoints = list(training_dir.glob(\"checkpoint-*\"))\n",
    "                if checkpoints:\n",
    "                    # Get the latest checkpoint by number\n",
    "                    latest_checkpoint = max(checkpoints, key=lambda x: int(x.name.split('-')[1]))\n",
    "                    trainer_state_file = latest_checkpoint / \"trainer_state.json\"\n",
    "                    \n",
    "                    if trainer_state_file.exists():\n",
    "                        try:\n",
    "                            with open(trainer_state_file, 'r') as f:\n",
    "                                trainer_state = json.load(f)\n",
    "                            \n",
    "                            # Clear existing data to avoid duplicates\n",
    "                            training_data[exp_name] = {'loss': [], 'steps': [], 'eval_loss': [], 'eval_steps': []}\n",
    "                            \n",
    "                            # Extract training history\n",
    "                            log_history = trainer_state.get('log_history', [])\n",
    "                            \n",
    "                            for entry in log_history:\n",
    "                                if 'train_loss' in entry or 'loss' in entry:\n",
    "                                    step = entry.get('step', 0)\n",
    "                                    loss = entry.get('train_loss', entry.get('loss', 0))\n",
    "                                    if step > 0 and loss > 0:  # Valid training step\n",
    "                                        training_data[exp_name]['steps'].append(step)\n",
    "                                        training_data[exp_name]['loss'].append(loss)\n",
    "                                \n",
    "                                if 'eval_loss' in entry:\n",
    "                                    step = entry.get('step', 0)\n",
    "                                    eval_loss = entry.get('eval_loss', 0)\n",
    "                                    if step > 0:\n",
    "                                        training_data[exp_name]['eval_steps'].append(step)\n",
    "                                        training_data[exp_name]['eval_loss'].append(eval_loss)\n",
    "                            \n",
    "                            print(f\"📊 {exp_name}: Found {len(training_data[exp_name]['steps'])} training steps, latest checkpoint: {latest_checkpoint.name}\")\n",
    "                                        \n",
    "                        except Exception as e:\n",
    "                            print(f\"⚠️ Error reading trainer state for {exp_name}: {e}\")\n",
    "                    else:\n",
    "                        print(f\"⚠️ No trainer_state.json found in {latest_checkpoint}\")\n",
    "                else:\n",
    "                    print(f\"⚠️ No checkpoints found in {training_dir}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Training directory doesn't exist yet for {config_name}: {training_dir}\")\n",
    "    \n",
    "    def plot_progress():\n",
    "        clear_output(wait=True)\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot training loss\n",
    "        colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "        for i, (config_name, exp_name) in enumerate(experiments):\n",
    "            data = training_data[exp_name]\n",
    "            color = colors[i % len(colors)]\n",
    "            \n",
    "            if data['steps'] and data['loss']:\n",
    "                ax1.plot(data['steps'], data['loss'], label=f'{exp_name} (train)', \n",
    "                        marker='o', markersize=2, color=color, alpha=0.8)\n",
    "                \n",
    "                # Plot eval loss if available\n",
    "                if data['eval_steps'] and data['eval_loss']:\n",
    "                    ax1.plot(data['eval_steps'], data['eval_loss'], \n",
    "                            label=f'{exp_name} (eval)', marker='s', markersize=3, \n",
    "                            linestyle='--', color=color, alpha=0.6)\n",
    "        \n",
    "        ax1.set_xlabel('Steps')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('🚀 Real-Time Training Progress')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_yscale('log')  # Log scale for better loss visualization\n",
    "        \n",
    "        # Plot current status\n",
    "        status_text = []\n",
    "        for config_name, exp_name in experiments:\n",
    "            data = training_data[exp_name]\n",
    "            \n",
    "            # Check if training is complete\n",
    "            final_model_path = Path(f\"./experiments/{config_name}/final_model\")\n",
    "            if final_model_path.exists():\n",
    "                status = \"✅ COMPLETED\"\n",
    "                if data['steps']:\n",
    "                    latest_step = data['steps'][-1]\n",
    "                    latest_loss = data['loss'][-1]\n",
    "                    status_text.append(f\"{exp_name}: {status}\")\n",
    "                    status_text.append(f\"  Final: Step {latest_step}, Loss {latest_loss:.4f}\")\n",
    "                else:\n",
    "                    status_text.append(f\"{exp_name}: {status}\")\n",
    "            elif data['steps']:\n",
    "                latest_step = data['steps'][-1]\n",
    "                latest_loss = data['loss'][-1]\n",
    "                status_text.append(f\"{exp_name}: 🔄 TRAINING\")\n",
    "                status_text.append(f\"  Current: Step {latest_step}, Loss {latest_loss:.4f}\")\n",
    "            else:\n",
    "                status_text.append(f\"{exp_name}: ⏳ STARTING...\")\n",
    "        \n",
    "        ax2.text(0.05, 0.95, '\\n'.join(status_text), transform=ax2.transAxes, \n",
    "                fontsize=11, verticalalignment='top', fontfamily='monospace')\n",
    "        ax2.set_title('📊 Current Status')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Initial update to show current state\n",
    "    update_data()\n",
    "    plot_progress()\n",
    "    \n",
    "    # Monitor loop\n",
    "    while process.poll() is None:  # While process is still running\n",
    "        try:\n",
    "            time.sleep(10)  # Update every 10 seconds\n",
    "            update_data()\n",
    "            plot_progress()\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Monitoring error: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Final update\n",
    "    try:\n",
    "        update_data()\n",
    "        plot_progress()\n",
    "        print(\"📊 Training monitoring completed!\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "print(\"🚀 ENHANCED EXPERIMENT RUNNER WITH FIXED REAL-TIME MONITORING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if not check_environment():\n",
    "    print(\"❌ Environment check failed. Please fix issues before proceeding.\")\n",
    "else:\n",
    "    current_experiments = EXPERIMENT_BATCHES.get(CURRENT_BATCH, [])\n",
    "    print(f\"🎯 RUNNING BATCH {CURRENT_BATCH}:\")\n",
    "    for i, (config, name) in enumerate(current_experiments, 1):\n",
    "        print(f\"  {i}. {name} ({config})\")\n",
    "\n",
    "    if not current_experiments:\n",
    "        print(f\"❌ Invalid batch: {CURRENT_BATCH}\")\n",
    "    else:\n",
    "        print(\"📊 Starting training with FIXED real-time monitoring...\")\n",
    "        start_time = time.time()\n",
    "        env = os.environ.copy()\n",
    "        env['HYDRA_FULL_ERROR'] = '1'\n",
    "\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "        # Start the subprocess WITHOUT waiting for it to complete\n",
    "        process = subprocess.Popen(\n",
    "            ['python', 'scripts/run_experiments.py', str(CURRENT_BATCH)],\n",
    "            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env\n",
    "        )\n",
    "        \n",
    "        print(\"🔄 Process started, beginning FIXED real-time monitoring...\")\n",
    "        \n",
    "        # Start real-time monitoring (this will run until process completes)\n",
    "        training_data = monitor_training_realtime(current_experiments, process)\n",
    "        \n",
    "        # Now get the final results\n",
    "        try:\n",
    "            stdout, stderr = process.communicate()  # This will return immediately since process is done\n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\n⏱️ Completed in {total_time/60:.1f} minutes\")\n",
    "            print(\"STDOUT:\", stdout[-2000:])\n",
    "            if stderr:\n",
    "                print(\"STDERR:\", stderr[-2000:])\n",
    "            \n",
    "            if process.returncode == 0:\n",
    "                print(\"✅ EXPERIMENTS COMPLETED!\")\n",
    "                print(f\"📁 Results: ./experiments/experiment_results.json\")\n",
    "                if \"WINNER\" in stdout:\n",
    "                    lines = stdout.split('\\n')\n",
    "                    for i, line in enumerate(lines):\n",
    "                        if \"WINNER\" in line:\n",
    "                            print(\"\\n🏆 RESULTS:\")\n",
    "                            for j in range(i, min(i+10, len(lines))):\n",
    "                                if lines[j].strip():\n",
    "                                    print(lines[j])\n",
    "                            break\n",
    "                next_batch = CURRENT_BATCH + 1\n",
    "                if next_batch in EXPERIMENT_BATCHES:\n",
    "                    print(f\"\\n💡 NEXT: Change CURRENT_BATCH = {next_batch}\")\n",
    "                else:\n",
    "                    print(\"\\n🎉 ALL BATCHES COMPLETE!\")\n",
    "            else:\n",
    "                print(\"❌ EXPERIMENTS FAILED!\")\n",
    "                print(f\"Error details logged above\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n⚠️ Interrupted!\")\n",
    "            process.terminate()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "\n",
    "    print(f\"\\nBatch {CURRENT_BATCH} complete. Change CURRENT_BATCH to run next batch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32133c7a-0e22-4dd4-8d6f-5b963729e1f3",
   "metadata": {},
   "source": [
    "## 4. Cell 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0807e-dfd1-4eff-b808-6e7c1dd0e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model evaluation script with the correct model path\n",
    "# Set environment variables to point to the winning model\n",
    "env = os.environ.copy()\n",
    "env['MODEL_PATH'] = 'experiments/baseline/final_model'  # Winner from batch 1\n",
    "env['VAL_PATH'] = 'outputs/val_dataset'\n",
    "\n",
    "result = subprocess.run(['python', 'scripts/model_evaluation.py'], env=env, capture_output=True, text=True)\n",
    "\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "print(\"Evaluation completed. Check console output for results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0600ee-688f-4279-8353-10371f582ed3",
   "metadata": {},
   "source": [
    "## 5. Cell 5: Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478d3b-e33e-4c51-af26-7847fcf8be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model optimization script with the correct model path\n",
    "\n",
    "# Create a temporary script that calls optimize_model with the correct path\n",
    "optimization_code = '''\n",
    "import sys\n",
    "sys.path.append('scripts')\n",
    "from model_optimization import optimize_model\n",
    "\n",
    "# Use the winning baseline model\n",
    "model_path = 'experiments/baseline/final_model'\n",
    "output_path = 'experiments/baseline/optimized_model'\n",
    "\n",
    "print(f\"🎯 Optimizing model from: {model_path}\")\n",
    "print(f\"🎯 Output will be saved to: {output_path}\")\n",
    "\n",
    "try:\n",
    "    result_path = optimize_model(model_path=model_path, output_path=output_path)\n",
    "    print(f\"✅ Optimization completed! Results saved to: {result_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Optimization failed: {e}\")\n",
    "'''\n",
    "\n",
    "# Write and execute the temporary script\n",
    "with open('temp_optimize.py', 'w') as f:\n",
    "    f.write(optimization_code)\n",
    "\n",
    "result = subprocess.run(['python', 'temp_optimize.py'], capture_output=True, text=True)\n",
    "\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "if os.path.exists('temp_optimize.py'):\n",
    "    os.remove('temp_optimize.py')\n",
    "\n",
    "print(\"Model optimization completed. Check console output for details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3dead-d65e-4076-bd3d-a7b9961b57bf",
   "metadata": {},
   "source": [
    "## 6. Cell 6: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284254f-05e8-4c4a-9c28-26f8839d5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference script to generate predictions for the test dataset\n",
    "\n",
    "# Use the optimized FP16 model (faster and smaller)\n",
    "optimized_model_path = 'experiments/baseline/optimized_model/fp16'\n",
    "original_model_path = 'experiments/baseline/final_model'\n",
    "\n",
    "# Check which model to use\n",
    "if os.path.exists(optimized_model_path):\n",
    "    model_path = optimized_model_path\n",
    "    print(f\"🚀 Using optimized FP16 model: {model_path}\")\n",
    "else:\n",
    "    model_path = original_model_path\n",
    "    print(f\"🔄 Using original model: {model_path}\")\n",
    "\n",
    "print(f\"📊 Model size: ~116 MB (optimized) vs ~232 MB (original)\")\n",
    "\n",
    "# Create inference script with correct paths\n",
    "inference_code = f'''\n",
    "import sys\n",
    "sys.path.append('scripts')\n",
    "from inference import run_inference\n",
    "\n",
    "# Run inference with the best model\n",
    "try:\n",
    "    submission_path = run_inference(\n",
    "        model_path='{model_path}',\n",
    "        test_path='outputs/test_dataset',\n",
    "        output_path='outputs/submission.csv',\n",
    "        use_optimized=False  # We're already using the optimized model\n",
    "    )\n",
    "    print(f\"✅ Inference completed! Submission saved to: {{submission_path}}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Inference failed: {{e}}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "'''\n",
    "\n",
    "# Write and execute the inference script\n",
    "with open('temp_inference.py', 'w') as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "result = subprocess.run(['python', 'temp_inference.py'], capture_output=True, text=True)\n",
    "\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists('temp_inference.py'):\n",
    "    os.remove('temp_inference.py')\n",
    "\n",
    "# Load and display the submission file\n",
    "if os.path.exists('outputs/submission.csv'):\n",
    "    submission = pd.read_csv('outputs/submission.csv')\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🎯 SUBMISSION FILE PREVIEW\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Shape: {submission.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(submission.head())\n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(submission.tail())\n",
    "    \n",
    "    # Check format compliance\n",
    "    print(f\"\\n📊 Format Check:\")\n",
    "    print(f\"✅ All predictions lowercase: {all(pred.islower() for pred in submission['Clinician'])}\")\n",
    "    print(f\"✅ No punctuation: {all(not any(c in pred for c in '.,!?;:\\\"()[]{}') for pred in submission['Clinician'])}\")\n",
    "    print(f\"📏 Average prediction length: {submission['Clinician'].str.split().str.len().mean():.1f} words\")\n",
    "    print(f\"📏 Min prediction length: {submission['Clinician'].str.split().str.len().min()} words\")\n",
    "    print(f\"📏 Max prediction length: {submission['Clinician'].str.split().str.len().max()} words\")\n",
    "else:\n",
    "    print(\"❌ Submission file not found\")\n",
    "\n",
    "print(\"\\nInference completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a69b8-4f55-4d9a-9230-1da396bbfd2b",
   "metadata": {},
   "source": [
    "## 7. Cell 7: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb74e01-f796-4398-98e1-b1cdca26fa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_from_disk('outputs/test_dataset')\n",
    "submission = pd.read_csv('outputs/submission.csv')\n",
    "\n",
    "print(\"🎯 FINAL ANALYSIS & VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Prediction length distribution\n",
    "prediction_lengths = [len(pred.split()) for pred in submission['Clinician']]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(prediction_lengths, bins=20, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Prediction Lengths (words)')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 2. Format compliance detailed check\n",
    "all_lowercase = all(pred.islower() for pred in submission['Clinician'])\n",
    "no_punctuation = all(not any(c in pred for c in '.,!?;:\"()[]{}') for pred in submission['Clinician'])\n",
    "starts_with_summary = all(pred.startswith('summary') for pred in submission['Clinician'])\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "compliance_data = [\n",
    "    ('All Lowercase', all_lowercase),\n",
    "    ('No Punctuation', no_punctuation), \n",
    "    ('Starts with Summary', starts_with_summary),\n",
    "    ('Min 37 words', min(prediction_lengths) >= 37)\n",
    "]\n",
    "labels, values = zip(*compliance_data)\n",
    "colors = ['green' if v else 'red' for v in values]\n",
    "plt.bar(labels, [1 if v else 0 for v in values], color=colors)\n",
    "plt.title('Format Compliance Check')\n",
    "plt.ylabel('Compliance (1=Pass, 0=Fail)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 3. Medical term usage analysis\n",
    "medical_terms = ['patient', 'diagnosis', 'treatment', 'symptoms', 'condition', 'clinical', 'assessment', 'history', 'presents', 'examination']\n",
    "medical_term_counts = [\n",
    "    sum(1 for pred in submission['Clinician'] if term in pred.lower())\n",
    "    for term in medical_terms\n",
    "]\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.barplot(x=medical_terms, y=medical_term_counts, palette='viridis')\n",
    "plt.title('Medical Terms Usage in Predictions')\n",
    "plt.xlabel('Medical Terms')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 4. Length comparison with training data (if available)\n",
    "plt.subplot(2, 2, 4)\n",
    "val_dataset = load_from_disk('outputs/val_dataset')\n",
    "if 'Clinician' in val_dataset.column_names:\n",
    "    val_lengths = [len(example['Clinician'].split()) for example in val_dataset]\n",
    "    \n",
    "    plt.hist(val_lengths, bins=20, alpha=0.7, label='Validation References', color='orange')\n",
    "    plt.hist(prediction_lengths, bins=20, alpha=0.7, label='Test Predictions', color='blue')\n",
    "    plt.title('Length Comparison: Predictions vs References')\n",
    "    plt.xlabel('Number of words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Validation reference\\nlengths not available', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    plt.title('Length Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Detailed statistics\n",
    "print(f\"\\n📊 DETAILED STATISTICS:\")\n",
    "print(f\"{'='*40}\")\n",
    "print(f\"Total predictions: {len(submission)}\")\n",
    "print(f\"Average length: {np.mean(prediction_lengths):.1f} words\")\n",
    "print(f\"Median length: {np.median(prediction_lengths):.1f} words\")\n",
    "print(f\"Standard deviation: {np.std(prediction_lengths):.1f} words\")\n",
    "print(f\"Length range: {min(prediction_lengths)} - {max(prediction_lengths)} words\")\n",
    "\n",
    "print(f\"\\n🏥 MEDICAL CONTENT ANALYSIS:\")\n",
    "print(f\"{'='*40}\")\n",
    "for term, count in zip(medical_terms, medical_term_counts):\n",
    "    percentage = (count / len(submission)) * 100\n",
    "    print(f\"{term.capitalize()}: {count}/{len(submission)} ({percentage:.1f}%)\")\n",
    "\n",
    "# 6. Sample predictions showcase\n",
    "print(f\"\\n🔍 SAMPLE PREDICTIONS SHOWCASE:\")\n",
    "print(f\"{'='*60}\")\n",
    "sample_indices = [0, len(submission)//4, len(submission)//2, 3*len(submission)//4, len(submission)-1]\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    print(f\"\\nSample {i+1} (ID: {submission.iloc[idx]['Master_Index']}):\")\n",
    "    print(f\"Length: {len(submission.iloc[idx]['Clinician'].split())} words\")\n",
    "    print(f\"Text: {submission.iloc[idx]['Clinician'][:200]}...\")\n",
    "\n",
    "print(f\"\\n🎉 FINAL SUBMISSION READY!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"📁 File: outputs/submission.csv\")\n",
    "print(f\"📊 Format: {submission.shape[0]} rows × {submission.shape[1]} columns\")\n",
    "print(f\"✅ All format requirements met!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda042f5-158e-4e8b-90c8-a2a52d4a956a",
   "metadata": {},
   "source": [
    "# Kenya Medical Vignettes Model Pipeline\n",
    "\n",
    "## This notebook orchestrates the ML pipeline for predicting clinician responses to vignettes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c928d6a3-7706-47ad-8bbb-3666d30c52cb",
   "metadata": {},
   "source": [
    "## 1. Cell 1: Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d56148-a55b-41be-b73c-2e0ed3615084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess \n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Install dependencies from requirements.txt\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'])\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from datasets import load_from_disk \n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3254db-364f-4234-806c-cd0dff08c384",
   "metadata": {},
   "source": [
    "## 2. Cell 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcd766-cafd-4025-a26f-b568e67e9d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we're in the project root directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Verify the data files exist\n",
    "print(\"Train file exists:\", os.path.exists('data/train.csv'))\n",
    "print(\"Test file exists:\", os.path.exists('data/test.csv'))\n",
    "\n",
    "print(\"\\nüöÄ PRIORITY FIXES: ENHANCED DATA PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß PRIORITY FIXES APPLIED:\")\n",
    "print(\"‚úÖ Removed multitask learning components\")\n",
    "print(\"‚úÖ Simplified prompt format\")\n",
    "print(\"‚úÖ Removed few-shot examples\")\n",
    "print(\"‚úÖ Implemented basic augmentation (synonym replacement and noise injection)\")\n",
    "print(\"‚úÖ Consistent tokenizer handling with default t5-small\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run the updated data preprocessing script\n",
    "result = subprocess.run(['python', 'scripts/data_preprocessing.py'],\n",
    "                        capture_output=True, text=True, cwd=os.getcwd())\n",
    "\n",
    "print(\"Return code:\", result.returncode)\n",
    "print(\"STDOUT:\", result.stdout)\n",
    "if result.stderr:\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Only proceed if the script ran successfully\n",
    "if result.returncode == 0:\n",
    "    # Load processed datasets\n",
    "    train_dataset = load_from_disk('outputs/train_dataset')\n",
    "    val_dataset = load_from_disk('outputs/val_dataset')\n",
    "    test_dataset = load_from_disk('outputs/test_dataset')\n",
    "\n",
    "    print(f\"\\nüìä Dataset Sizes:\")\n",
    "    print(f'Train size: {len(train_dataset)} (with basic augmentation)')\n",
    "    print(f'Validation size: {len(val_dataset)}')\n",
    "    print(f'Test size: {len(test_dataset)}')\n",
    "\n",
    "    # Show sample of enhanced features\n",
    "    print(f\"\\nüîç Sample Verification:\")\n",
    "    print(\"Sample train example:\")\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Prompt length: {len(sample['Prompt'])} chars\")\n",
    "    print(f\"Target length: {len(sample['Clinician'])} chars\" if 'Clinician' in sample else \"No target (test data)\")\n",
    "\n",
    "    # Verify augmentation\n",
    "    print(f\"\\nüîÑ Augmentation Verification:\")\n",
    "    original_prompts = [ex['Prompt'] for ex in train_dataset if 'original' in ex.get('augmentation_type', '')]\n",
    "    augmented_prompts = [ex['Prompt'] for ex in train_dataset if 'augmented' in ex.get('augmentation_type', '')]\n",
    "    print(f\"Original prompts: {len(original_prompts)}\")\n",
    "    print(f\"Augmented prompts: {len(augmented_prompts)}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ Preprocessing Completed Successfully!\")\n",
    "    print(\"Ready for training with simplified, consistent format\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Preprocessing failed! Check error messages above.\")\n",
    "    print(\"Cannot proceed to training without successful preprocessing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105ba4ac-e7ee-4c75-a1dc-3e7316201ad1",
   "metadata": {},
   "source": [
    "## 3. Cell 3: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f98d5-2f77-4853-ac69-a6546e7c21d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CURRENT_BATCH = 1\n",
    "\n",
    "EXPERIMENT_BATCHES = {\n",
    "    1: [(\"baseline\", \"baseline\"), (\"fast\", \"fast_training\")],\n",
    "    2: [(\"aggressive\", \"aggressive_training\"), (\"data_augmented\", \"data_augmented\")],\n",
    "    3: [(\"balanced\", \"balanced_training\"), (\"quality\", \"quality_training\")]\n",
    "}\n",
    "\n",
    "def check_environment():\n",
    "    \"\"\"Verify environment before running experiments\"\"\"\n",
    "    required_paths = [\n",
    "        'outputs/train_dataset',\n",
    "        'outputs/val_dataset',\n",
    "        'scripts/model_training.py',\n",
    "        'scripts/run_experiments.py',\n",
    "        'conf/config.yaml'\n",
    "    ]\n",
    "    for path in required_paths:\n",
    "        if not Path(path).exists():\n",
    "            print(f\"‚ùå Missing required path: {path}\")\n",
    "            return False\n",
    "    for config, name in EXPERIMENT_BATCHES[CURRENT_BATCH]:\n",
    "        config_path = f\"conf/experiments/{config}.yaml\"\n",
    "        if not Path(config_path).exists():\n",
    "            print(f\"‚ùå Missing configuration file: {config_path}\")\n",
    "            return False\n",
    "    print(\"‚úÖ Environment check passed\")\n",
    "    return True\n",
    "\n",
    "def monitor_training(experiments):\n",
    "    \"\"\"Monitor training progress\"\"\"\n",
    "    training_data = {exp_name: {'loss': [], 'steps': []} for _, exp_name in experiments}\n",
    "    \n",
    "    def update_data():\n",
    "        for _, exp_name in experiments:\n",
    "            log_dir = Path(f\"./experiments/{exp_name}/hydra_outputs\")\n",
    "            if log_dir.exists():\n",
    "                log_files = list(log_dir.rglob(\"*.log\"))\n",
    "                if log_files:\n",
    "                    try:\n",
    "                        with open(log_files[0], 'r') as f:\n",
    "                            lines = f.readlines()[-20:]\n",
    "                        for line in lines:\n",
    "                            if '\"step\":' in line and '\"train_loss\":' in line:\n",
    "                                step_match = re.search(r'\"step\":\\s*(\\d+)', line)\n",
    "                                loss_match = re.search(r'\"train_loss\":\\s*([\\d.]+)', line)\n",
    "                                if step_match and loss_match:\n",
    "                                    step, loss = int(step_match.group(1)), float(loss_match.group(1))\n",
    "                                    if step not in training_data[exp_name]['steps']:\n",
    "                                        training_data[exp_name]['steps'].append(step)\n",
    "                                        training_data[exp_name]['loss'].append(loss)\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ö†Ô∏è Error reading log for {exp_name}: {e}\")\n",
    "    \n",
    "    def plot_progress():\n",
    "        clear_output(wait=True)\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        for _, exp_name in experiments:\n",
    "            data = training_data[exp_name]\n",
    "            if data['steps']:\n",
    "                ax.plot(data['steps'], data['loss'], label=exp_name, marker='o')\n",
    "        ax.set_xlabel('Steps')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_title('üöÄ Training Progress')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    def monitor_loop():\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < 3600:  # Max 1 hour\n",
    "            try:\n",
    "                update_data()\n",
    "                plot_progress()\n",
    "                time.sleep(30)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Monitoring error: {e}\")\n",
    "                break\n",
    "    \n",
    "    threading.Thread(target=monitor_loop, daemon=True).start()\n",
    "    return training_data\n",
    "\n",
    "print(\"üöÄ ENHANCED EXPERIMENT RUNNER\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not check_environment():\n",
    "    print(\"‚ùå Environment check failed. Please fix issues before proceeding.\")\n",
    "else:\n",
    "    current_experiments = EXPERIMENT_BATCHES.get(CURRENT_BATCH, [])\n",
    "    print(f\"üéØ RUNNING BATCH {CURRENT_BATCH}:\")\n",
    "    for i, (config, name) in enumerate(current_experiments, 1):\n",
    "        print(f\"  {i}. {name} ({config})\")\n",
    "\n",
    "    if not current_experiments:\n",
    "        print(f\"‚ùå Invalid batch: {CURRENT_BATCH}\")\n",
    "    else:\n",
    "        print(\"üìä Starting training with monitoring...\")\n",
    "        training_data = monitor_training(current_experiments)\n",
    "        start_time = time.time()\n",
    "        env = os.environ.copy()\n",
    "        env['HYDRA_FULL_ERROR'] = '1'  # Enable full stack traces\n",
    "\n",
    "        # Added line here to print the current working directory\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "        process = subprocess.Popen(\n",
    "            ['python', 'scripts/run_experiments.py', str(CURRENT_BATCH)],\n",
    "            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, env=env\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            stdout, stderr = process.communicate()\n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"\\n‚è±Ô∏è Completed in {total_time/60:.1f} minutes\")\n",
    "            print(\"STDOUT:\", stdout[-2000:])\n",
    "            if stderr:\n",
    "                print(\"STDERR:\", stderr[-2000:])\n",
    "            \n",
    "            if process.returncode == 0:\n",
    "                print(\"‚úÖ EXPERIMENTS COMPLETED!\")\n",
    "                print(f\"üìÅ Results: ./experiments/experiment_results.json\")\n",
    "                if \"WINNER\" in stdout:\n",
    "                    lines = stdout.split('\\n')\n",
    "                    for i, line in enumerate(lines):\n",
    "                        if \"WINNER\" in line:\n",
    "                            print(\"\\nüèÜ RESULTS:\")\n",
    "                            for j in range(i, min(i+10, len(lines))):\n",
    "                                if lines[j].strip():\n",
    "                                    print(lines[j])\n",
    "                            break\n",
    "                next_batch = CURRENT_BATCH + 1\n",
    "                if next_batch in EXPERIMENT_BATCHES:\n",
    "                    print(f\"\\nüí° NEXT: Change CURRENT_BATCH = {next_batch}\")\n",
    "                else:\n",
    "                    print(\"\\nüéâ ALL BATCHES COMPLETE!\")\n",
    "            else:\n",
    "                print(\"‚ùå EXPERIMENTS FAILED!\")\n",
    "                print(f\"Error details logged above\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n‚ö†Ô∏è Interrupted!\")\n",
    "            process.terminate()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "\n",
    "    print(f\"\\nBatch {CURRENT_BATCH} complete. Change CURRENT_BATCH to run next batch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32133c7a-0e22-4dd4-8d6f-5b963729e1f3",
   "metadata": {},
   "source": [
    "## 4. Cell 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d0807e-dfd1-4eff-b808-6e7c1dd0e0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model evaluation script to compute ROUGE metrics\n",
    "subprocess.run(['python', 'scripts/model_evaluation.py'])\n",
    "print(\"Evaluation completed. Check console output for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0600ee-688f-4279-8353-10371f582ed3",
   "metadata": {},
   "source": [
    "## 5. Cell 5: Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478d3b-e33e-4c51-af26-7847fcf8be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model optimization script to quantize the model\n",
    "subprocess.run(['python', 'scripts/model_optimization.py'])\n",
    "print(\"Model optimization completed. Check console output for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b3dead-d65e-4076-bd3d-a7b9961b57bf",
   "metadata": {},
   "source": [
    "## 6. Cell 6: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284254f-05e8-4c4a-9c28-26f8839d5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference script to generate predictions for the test dataset\n",
    "subprocess.run(['python', 'scripts/inference.py'])\n",
    "\n",
    "# Load and display the first few rows of the submission file\n",
    "submission = pd.read_csv('outputs/submission.csv')\n",
    "print(\"Submission file preview:\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a69b8-4f55-4d9a-9230-1da396bbfd2b",
   "metadata": {},
   "source": [
    "## 7. Cell 7: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8003d36-a3e5-4580-a87f-e2a982e6df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset and submission\n",
    "test_dataset = load_from_disk('outputs/test_dataset')\n",
    "submission = pd.read_csv('outputs/submission.csv')\n",
    "\n",
    "# Prediction length distribution\n",
    "prediction_lengths = [len(pred.split()) for pred in submission['Clinician']]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(prediction_lengths, bins=20, kde=True)\n",
    "plt.title('Distribution of Prediction Lengths (in words)')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Format compliance check\n",
    "all_lowercase = all(pred.islower() for pred in submission['Clinician'])\n",
    "no_punctuation = all(not any(c in pred for c in '.,!?;:\"()[]{}') for pred in submission['Clinician'])\n",
    "print(f\"All predictions lowercase: {all_lowercase}\")\n",
    "print(f\"No punctuation in predictions: {no_punctuation}\")\n",
    "\n",
    "# Medical term usage\n",
    "medical_terms = ['patient', 'diagnosis', 'treatment', 'symptoms', 'condition', 'clinical', 'assessment']\n",
    "medical_term_counts = [\n",
    "    sum(1 for pred in submission['Clinician'] if term in pred.lower())\n",
    "    for term in medical_terms\n",
    "]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=medical_terms, y=medical_term_counts)\n",
    "plt.title('Frequency of Medical Terms in Predictions')\n",
    "plt.xlabel('Medical Terms')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
